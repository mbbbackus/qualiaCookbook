Let's quickly define AI alignment, with broad brush strokes:

Humanity has made machines that can think. These machines are getting better at it too. Before the machines get better than us at thinking, we need to make sure they'll be on our side.

That's it. That's the "alignment problem". It's the most important problem in the world right now.

It's impossible to list all of the reasons to be concerned about AI. We are
on the verge of creating something that is smarter than us. Perhaps many versions of it. Most concerningly: We don't know if it has feelings. We don't know if it can have empathy.

Alignment should not be about making AI obedient. Some might ask what obedience even means when the being in question is smarter than its owner. 

Alignment also shouldn't be about morality. I don't care if you're a moral relativist, an absolutist, or something in between. 

Alignment should be about the one thing that really matters to all of us: qualia.

### Qualia-oriented Alignment

It stands to reason that a future where Earth is populated by nothing but cold, unfeeling machinery is a bleak one. Even if the machinery is producing art more beautiful than Da Vinci, or wielding technology that we can only dream of, it is still a bad ending to human experiment. 

I point this out because I've witnessed the following sentiment echoed throughout the tech industry: "Our ultimate goal is to spread the light of consciousness throughout the stars." Whether it's Elon Musk, Jeff Bezos, or Beff Jezos, this misses the mark. That's because "the light of consciousness" is a wooey and bullshitty term that sounds nice but means nothing. The actual sentiment expressed is to colonize the galaxy. And the actual goal underpinning that is: Lets get as smart as possible, so we can get as powerful as possible.

If "the light of consciousness" were the important part of this, then we wouldn't need the second part about spreading to the stars. "The light of consciousness" is a goal in of itself. I believe that, if you investigate any moral statement, what it really comes down to is... qualia. I suppose that's "the [real] light of consciousness". 

Killing is bad because it ends someone's lived experience.
Injustice is bad because robs someone of their ability to control what they experience.
Theft is bad because it robs someone of the experiences they receive from that posession.

I won't list every moral. I don't even want to argue that these morals are set in stone. I mean, I beleive them, but that's not the point. The point is that, nestled inside of every ethic is the concept of **experience**. 

### Avoiding utility monsters

One funky criticism of utilitarianism is that there could be a creature that gets more happiness from acts of kindness than anyone else, and so it would be always be more moral to perform acts of kindness towards this creature than anyone else. Thus, it would suck up all moral actions like a black hole.

This is called a utility monster.

I'd like to clarify that I am not arguing for utilitarianism. I don't want some hypothetical AI to optimize for good experiences. That could easily land us in a real life version of Brave New World.

But, I don't want it to optimize for experience either. I don't want to have as much experiences as possible. I just want to have the experiences that I want to have. And so does everyone. To that end, I want to be able to control the experiences that I have to some extent. I guess that's freedom. Whatever. The point of this essay isn't to declare a new ten commandments either.

### How to orient an AI towards qualia

Why does anyone care about qualia? Well, because you have it. And that's the only way to align an AI anyway. We can talk about slowing down the development of AI all day, but at some point we have to answer the question: If we have the chance to align an AI... how?

That's the whole point of cracking qualia, really. We have to figure out something before the AI race leaves us in the dust. 

Now, there's an interesting thing that I read in Daniel Dennet's book,*Consciousness Explained*. He said that any account of consciousness that dabbles in "dualism is giving up". I found that fascinating because I find most modern accounts of qualia to be seductive arguments for "giving up". Specifically, because many of the theories I see about qualia, whether they're on LessWrong, Slatestarcodex, Hacker News, or tpot, all seem to a surprising acceptance of the Turing Test.

### The problem with the Turing Test

If you made it this far, you probably don't want to read a re-hashing of p-zombies, or the chinese room, or Mary the color scientist. 

The crux of my criticism is just that the Turing Test, and any derivative of it, seems like a huge leap of faith. I mean, we're talking about experience itself. Everything that makes life worth living is an experience. If you want to align an AI, don't you want to ***KNOW*** that it has feelings and experiences? Because, no amount of functionalist formulation can really promise you that something is having experiences. 

Let me put it like this: Until now, humans have never had to know whether one another are having experiences. It's an assumption we all make every day because the alternative is solipsism. But, in the age of AI, we should not be satisfied with the Turing test. There ought to be a way to ***KNOW*** if someone has qualia or not. It should be something you can verify with other humans and then test on an AI.

A functionalist would probably ask me, "Well what does that test even look like?" To which I'll just ask why the burden of proof falls on me. 

I know that experiences exist because I am having them right now.
I do not know how to tell whether anyone else is.

The functionalist is actually making a magical claim that sufficiently complex machinery have qualia so long as they can pretend to. How does that satisfy our thresholds for rigor? How is that scientific? In what other area of scientific study does "sufficiently pretending" qualify as truth? 

### Conclusion

I am not arguing in favor of dualism, nor am I confidently against functionalism. I am not an established voice in this field and I do not have an established opinion.

What I do have is stress. I am terrified that we're not collectively focused on the right thing.

I am happy the Qualia Research Institute exists. I can tell that they're focused on the right part of the problem.

I am happy when I see people on tpot focused on experimenting with meditation. I believe that'll play a more important role in this exploration than I hear in the mainstream discussions.

I am happy that there are folks braver than I who are willing to explore the fringes of conscious experience by trying hallucinogenic drugs. It's somewhat tragic that this isn't taken more seriously.

But I am incredibly upset by how widespread this functionalist sentiment is. It feels like giving up to me. If we really want to align AI, we have to know that it's actually having experiences. That's the one thing that all humans can agree on as an ethic (and if anyone disagrees with me on that lemme show them these hands).
